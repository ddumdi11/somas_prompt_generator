"""LinkedIn-Formatter fÃ¼r Unicode-basierte Textformatierung.

LinkedIn unterstÃ¼tzt kein Markdown, aber Unicode-Zeichen fÃ¼r Fett/Kursiv.
"""

import re

# Unicode Bold (Sans-Serif Bold)
UNICODE_BOLD = {
    'A': 'ð—”', 'B': 'ð—•', 'C': 'ð—–', 'D': 'ð——', 'E': 'ð—˜', 'F': 'ð—™', 'G': 'ð—š',
    'H': 'ð—›', 'I': 'ð—œ', 'J': 'ð—', 'K': 'ð—ž', 'L': 'ð—Ÿ', 'M': 'ð— ', 'N': 'ð—¡',
    'O': 'ð—¢', 'P': 'ð—£', 'Q': 'ð—¤', 'R': 'ð—¥', 'S': 'ð—¦', 'T': 'ð—§', 'U': 'ð—¨',
    'V': 'ð—©', 'W': 'ð—ª', 'X': 'ð—«', 'Y': 'ð—¬', 'Z': 'ð—­',
    'a': 'ð—®', 'b': 'ð—¯', 'c': 'ð—°', 'd': 'ð—±', 'e': 'ð—²', 'f': 'ð—³', 'g': 'ð—´',
    'h': 'ð—µ', 'i': 'ð—¶', 'j': 'ð—·', 'k': 'ð—¸', 'l': 'ð—¹', 'm': 'ð—º', 'n': 'ð—»',
    'o': 'ð—¼', 'p': 'ð—½', 'q': 'ð—¾', 'r': 'ð—¿', 's': 'ð˜€', 't': 'ð˜', 'u': 'ð˜‚',
    'v': 'ð˜ƒ', 'w': 'ð˜„', 'x': 'ð˜…', 'y': 'ð˜†', 'z': 'ð˜‡',
    '0': 'ðŸ¬', '1': 'ðŸ­', '2': 'ðŸ®', '3': 'ðŸ¯', '4': 'ðŸ°',
    '5': 'ðŸ±', '6': 'ðŸ²', '7': 'ðŸ³', '8': 'ðŸ´', '9': 'ðŸµ',
    # Umlaute haben keine Unicode-Bold-Varianten, bleiben normal
}

# Unicode Italic (Sans-Serif Italic)
UNICODE_ITALIC = {
    'A': 'ð˜ˆ', 'B': 'ð˜‰', 'C': 'ð˜Š', 'D': 'ð˜‹', 'E': 'ð˜Œ', 'F': 'ð˜', 'G': 'ð˜Ž',
    'H': 'ð˜', 'I': 'ð˜', 'J': 'ð˜‘', 'K': 'ð˜’', 'L': 'ð˜“', 'M': 'ð˜”', 'N': 'ð˜•',
    'O': 'ð˜–', 'P': 'ð˜—', 'Q': 'ð˜˜', 'R': 'ð˜™', 'S': 'ð˜š', 'T': 'ð˜›', 'U': 'ð˜œ',
    'V': 'ð˜', 'W': 'ð˜ž', 'X': 'ð˜Ÿ', 'Y': 'ð˜ ', 'Z': 'ð˜¡',
    'a': 'ð˜¢', 'b': 'ð˜£', 'c': 'ð˜¤', 'd': 'ð˜¥', 'e': 'ð˜¦', 'f': 'ð˜§', 'g': 'ð˜¨',
    'h': 'ð˜©', 'i': 'ð˜ª', 'j': 'ð˜«', 'k': 'ð˜¬', 'l': 'ð˜­', 'm': 'ð˜®', 'n': 'ð˜¯',
    'o': 'ð˜°', 'p': 'ð˜±', 'q': 'ð˜²', 'r': 'ð˜³', 's': 'ð˜´', 't': 'ð˜µ', 'u': 'ð˜¶',
    'v': 'ð˜·', 'w': 'ð˜¸', 'x': 'ð˜¹', 'y': 'ð˜º', 'z': 'ð˜»',
    # Umlaute haben keine Unicode-Italic-Varianten, bleiben normal
}


def to_bold(text: str) -> str:
    """Konvertiert Text zu Unicode Bold.

    Args:
        text: Eingabetext

    Returns:
        Text mit Unicode Bold-Zeichen
    """
    return ''.join(UNICODE_BOLD.get(c, c) for c in text)


def to_italic(text: str) -> str:
    """Konvertiert Text zu Unicode Italic.

    Args:
        text: Eingabetext

    Returns:
        Text mit Unicode Italic-Zeichen
    """
    return ''.join(UNICODE_ITALIC.get(c, c) for c in text)


def create_post_header(
    title: str, channel: str,
    model_name: str = "", provider_name: str = "",
) -> str:
    """Erstellt den LinkedIn-Post-Header aus Video-Metadaten.

    Format:
        ð—©ð—¶ð—±ð—²ð—¼-ð—§ð—¶ð˜ð—²ð—¹ (fett)
        Kanal, YT

        ð—¦ð—¢ð— ð—”ð—¦-ð—”ð—»ð—®ð—¹ð˜†ð˜€ð—² (fett)
        via Modell, Provider (optional)

    Args:
        title: Video-Titel
        channel: Kanal-Name
        model_name: Optional - Name des verwendeten Modells
        provider_name: Optional - Name des API-Providers

    Returns:
        Formatierter Header fÃ¼r LinkedIn-Post
    """
    bold_title = to_bold(title)
    bold_somas = to_bold("SOMAS-Analyse")
    header = f"{bold_title}\n{channel}, YT\n\n{bold_somas}\n"
    if model_name and provider_name:
        header += f"via {model_name}, {provider_name}\n"
    header += "\n"
    return header


def extract_analysis_body(text: str) -> str:
    """Extrahiert den Analyse-Text ab FRAMING (ohne Einleitung).

    Args:
        text: VollstÃ¤ndiger Analyse-Text

    Returns:
        Text ab FRAMING (ohne EinleitungssÃ¤tze)
    """
    # Suche nach FRAMING (mit oder ohne ### Markdown-Header)
    framing_patterns = [
        r'(?m)^###?\s*FRAMING',  # ### FRAMING oder # FRAMING
        r'(?m)^FRAMING',          # FRAMING ohne Markdown
        r'(?m)^ð—™ð—¥ð—”ð— ð—œð—¡ð—š',        # Bereits konvertiertes Unicode-Bold
    ]

    for pattern in framing_patterns:
        match = re.search(pattern, text)
        if match:
            return text[match.start():]

    # Falls kein FRAMING gefunden, gib den ganzen Text zurÃ¼ck
    return text


def strip_url_protocol(url: str) -> str:
    """Entfernt Protokoll und www. von einer URL.

    Args:
        url: VollstÃ¤ndige URL

    Returns:
        URL ohne https://, http:// und www.
    """
    url = re.sub(r'^https?://', '', url)
    url = re.sub(r'^www\.', '', url)
    return url


def extract_domain_name(url: str) -> str:
    """Extrahiert den Domain-Namen ohne Protokoll, www und TLD.

    Args:
        url: VollstÃ¤ndige URL oder Domain

    Returns:
        Reiner Domain-Name (z.B. 'timesofisrael', 'cnn')
    """
    domain = strip_url_protocol(url).split('/')[0]
    # Compound-TLDs entfernen (.co.uk, .com.au, .org.uk, etc.)
    stripped = re.sub(
        r'\.(co|com|org|net|gov)\.[a-z]{2}$', '', domain, flags=re.IGNORECASE
    )
    if stripped != domain:
        domain = stripped
    else:
        # Einfache TLD entfernen (.com, .org, .net, .de, etc.)
        domain = re.sub(r'\.[a-z]{2,}$', '', domain, flags=re.IGNORECASE)
    return domain


def format_for_linkedin(
    text: str, video_title: str = "", video_channel: str = "",
    model_name: str = "", provider_name: str = "",
    citations: list[str] | None = None,
) -> tuple[str, str]:
    """Konvertiert Markdown-formatierten Text zu LinkedIn-kompatiblem Format.

    Transformationen:
    - Extrahiert nur den Analyse-Teil (ab FRAMING)
    - ### HEADING â†’ entfernt (ZwischenÃ¼berschriften werden zu Leerzeilen)
    - **bold** â†’ ð—¯ð—¼ð—¹ð—±
    - *italic* oder _italic_ â†’ ð˜ªð˜µð˜¢ð˜­ð˜ªð˜¤
    - - item â†’ â€¢ item
    - URLs aus FlieÃŸtext entfernen, am Ende als Quellenblock sammeln

    Args:
        text: Markdown-formatierter Text
        video_title: Optional - Video-Titel fÃ¼r Post-Header
        video_channel: Optional - Kanal-Name fÃ¼r Post-Header
        model_name: Optional - Name des verwendeten Modells
        provider_name: Optional - Name des API-Providers
        citations: Optional - Liste von Quell-URLs (z.B. von Perplexity API).
            Wenn vorhanden, werden diese als Quellen Ã¼bernommen. Die [N]-Marker
            im Text (von der API gesetzt) bleiben erhalten.

    Returns:
        Tuple aus (LinkedIn-Text, Detail-Quellen).
        LinkedIn-Text: Formatierter Text mit Unicode und Kurzquellen.
        Detail-Quellen: Nummerierte Quellenliste mit vollen URLs.
    """
    # Nur den Analyse-Teil ab FRAMING extrahieren (ohne Einleitung)
    analysis_text = extract_analysis_body(text)

    lines = analysis_text.split('\n')
    result_lines: list[str] = []
    collected_sources: list[tuple[int, str, str, str]] = []  # (nr, name, url, domain)
    footnote_counter = 0

    # API-Citations vorbelegen (z.B. Perplexity gibt URLs separat zurÃ¼ck,
    # der Text enthÃ¤lt bereits [1][2]-Marker ohne die eigentlichen URLs)
    if citations:
        for i, url in enumerate(citations, start=1):
            domain = extract_domain_name(url)
            collected_sources.append((i, domain, url, domain))
            footnote_counter = i

    # SOMAS-AbschnittsÃ¼berschriften (mit und ohne Markdown-Hashes)
    somas_headers = [
        'FRAMING', 'KERNTHESE', 'ELABORATION', 'IMPLIKATION',
        'KRITIK', 'OFFENE_FRAGEN', 'ZITATE', 'VERBINDUNGEN',
        'ANSCHLUSSFRAGE', 'QUICK INFO'
    ]
    somas_pattern = r'^(?:#{1,6}\s+)?(' + '|'.join(somas_headers) + r')(?:\s*:?)?\s*$'

    for line in lines:
        # SOMAS-Ãœberschriften â†’ Leerzeile (Abschnitt visuell trennen)
        if re.match(somas_pattern, line.strip(), re.IGNORECASE):
            # FÃ¼ge Leerzeile als Trenner hinzu (falls nicht am Anfang)
            if result_lines and result_lines[-1].strip():
                result_lines.append('')
            continue

        # Andere Markdown-Headers: ### HEADING â†’ auch entfernen
        header_match = re.match(r'^(#{1,6})\s+(.+)$', line)
        if header_match:
            if result_lines and result_lines[-1].strip():
                result_lines.append('')
            continue

        # SOMAS-Header am Zeilenanfang entfernen (auch mit nachfolgendem Text)
        for header in somas_headers:
            line = re.sub(rf'^{header}\s*:\s*', '', line, flags=re.IGNORECASE)

        # Markdown Links: [text](url) â†’ Text [N]
        def collect_markdown_link(match: re.Match[str]) -> str:
            nonlocal footnote_counter
            name = match.group(1)
            url = match.group(2)
            footnote_counter += 1
            domain = extract_domain_name(url)
            collected_sources.append((footnote_counter, name, url, domain))
            return f"{name} [{footnote_counter}]"
        line = re.sub(r'\[([^\]]+)\]\(([^)]+)\)', collect_markdown_link, line)

        # Bare URLs im Text: durch [N] ersetzen
        def collect_bare_url(match: re.Match[str]) -> str:
            nonlocal footnote_counter
            url = match.group(0).rstrip('.,!?;:')
            domain = extract_domain_name(url)
            footnote_counter += 1
            collected_sources.append((footnote_counter, domain, url, domain))
            return f"[{footnote_counter}]"
        line = re.sub(r'https?://[^\s,)]+', collect_bare_url, line)

        # Code blocks: `code` â†’ code (einfach Backticks entfernen)
        line = re.sub(r'`([^`]+)`', r'\1', line)

        # Bold: **text** â†’ Unicode Bold
        def bold_replace(match):
            return to_bold(match.group(1))
        line = re.sub(r'\*\*([^*]+)\*\*', bold_replace, line)

        # Italic: *text* oder _text_ â†’ Unicode Italic
        def italic_replace(match):
            return to_italic(match.group(1))
        line = re.sub(r'(?<!\*)\*([^*]+)\*(?!\*)', italic_replace, line)
        line = re.sub(r'_([^_]+)_', italic_replace, line)

        # Bullet points: - item â†’ â€¢ item
        line = re.sub(r'^(\s*)-\s+', r'\1â€¢ ', line)

        result_lines.append(line)

    formatted_text = '\n'.join(result_lines)

    # Mehrfache Leerzeilen auf eine reduzieren
    formatted_text = re.sub(r'\n{3,}', '\n\n', formatted_text)

    # FÃ¼hrende/trailing Leerzeilen entfernen
    formatted_text = formatted_text.strip()

    # Post-Header hinzufÃ¼gen, wenn Video-Infos vorhanden
    if video_title and video_channel:
        header = create_post_header(
            video_title, video_channel, model_name, provider_name
        )
        formatted_text = header + formatted_text

    # Domain-Namen vor [N]-Markern entfernen (AI gibt oft "domainname URL" aus)
    if collected_sources:
        for _, _, _url, domain in collected_sources:
            # "domainname [N]" â†’ "[N]" und "domainname. [N]" â†’ "[N]"
            formatted_text = re.sub(
                rf'\b{re.escape(domain)}\.?\s*(\[\d+\])',
                r'\1',
                formatted_text,
                flags=re.IGNORECASE
            )

    # Quellenblock am Ende: gleiche Domains zusammenfassen
    if collected_sources:
        # Gruppiere FuÃŸnoten-Nummern nach Domain
        domain_numbers: dict[str, list[int]] = {}
        for number, _name, _url, domain in collected_sources:
            if domain not in domain_numbers:
                domain_numbers[domain] = []
            domain_numbers[domain].append(number)

        # Formatiere: "1,6: timesofisrael" oder "2: cnn"
        source_parts: list[str] = []
        for domain, numbers in domain_numbers.items():
            nums = ",".join(str(n) for n in numbers)
            source_parts.append(f"{nums}: {domain}")

        formatted_text += "\n\nQuellen: " + " | ".join(source_parts)

    # Detail-Quellen: nummerierte Liste mit vollen URLs
    detailed_sources = ""
    if collected_sources:
        detail_lines: list[str] = ["Quellenangaben im Detail:"]
        for number, name, url, _domain in collected_sources:
            detail_lines.append(f"[{number}] {name} - {url}")
        detailed_sources = "\n".join(detail_lines)

    return formatted_text, detailed_sources
